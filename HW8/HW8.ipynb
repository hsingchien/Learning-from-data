{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn.svm kit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10588396653408316, 0.10026059525442321, 0.089425318886298122, 0.091071183651076693, 0.074338225209162001]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "train = pd.read_excel('train.xlsx')\n",
    "test = pd.read_excel('test.xlsx')\n",
    "train.columns = ['dig','inten','symm']\n",
    "test.columns = ['dig','inten','symm']\n",
    "\n",
    "\n",
    "def one_vs_all(tar, train, C, Q):\n",
    "\ttrain_c = train.copy()\n",
    "\tY = np.zeros(train_c.shape[0])\n",
    "\tY[train_c.dig == tar] = 1 # Y is the label\n",
    "\tX = train.iloc[0:, 1:3] # X is the input\n",
    "\tgram = (1+np.dot(X, X.T))**Q ## pre-compute the kernel\n",
    "\tclf = svm.SVC(C=0.01, kernel = \"precomputed\")\n",
    "\tclf.fit(gram, Y)\n",
    "\treturn [clf,gram,Y]\n",
    "err = []\n",
    "for i in list([0,2,4,6,8]):\n",
    "\tfit = one_vs_all(i,train,0.01,2)\n",
    "\tclf = fit[0]\n",
    "\tY = fit[2]\n",
    "\tgram = fit[1]\n",
    "\ttrain_pred = clf.predict(gram)\n",
    "\terror = 1-np.sum(np.sign(train_pred) == np.sign(Y))/train_pred.shape[0]\n",
    "\terr.append(error)\n",
    "print(err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. As shown in the output, 0 has the highest $E_{in}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.014401316691811772, 0.090248251268687407, 0.076258400768070222, 0.088465231106844011, 0.088328075709779186]\n"
     ]
    }
   ],
   "source": [
    "err = []\n",
    "for i in np.arange(1,11,2):\n",
    "\tfit = one_vs_all(i,train,0.01,2)\n",
    "\tclf = fit[0]\n",
    "\tY = fit[2]\n",
    "\tgram = fit[1]\n",
    "\ttrain_pred = clf.predict(gram)\n",
    "\terror = 1-np.sum(np.sign(train_pred) == np.sign(Y))/train_pred.shape[0]\n",
    "\terr.append(error)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. As shown in the output, 1 has the lowest $E_{in}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in sample error [0.004484304932735439, 0.004484304932735439, 0.004484304932735439, 0.0032030749519538215]\n",
      "number of supporter vectors [76, 34, 24, 24]\n",
      "out of sample error [0.01650943396226412, 0.018867924528301883, 0.018867924528301883, 0.018867924528301883]\n"
     ]
    }
   ],
   "source": [
    "err = []\n",
    "sv = []\n",
    "err_out = []\n",
    "def one_vs_one(tar1, tar2, train, Co, Q):\n",
    "\tX = train[train['dig'].isin([tar1,tar2])]\n",
    "\tY = np.zeros(X.shape[0])\n",
    "\tY[X.dig == tar1] = 1 # set up Y\n",
    "\tX = X.iloc[0:, 1:3]\n",
    "\tgram = (1+np.dot(X, X.T))**Q ## pre-compute the kernel\n",
    "\tclf = svm.SVC(C = Co, kernel = \"precomputed\")\n",
    "\tclf.fit(gram, Y)\n",
    "\treturn clf\n",
    "def error_compute(clf, train, test, Q):\n",
    "\tXout = Xout = test[test['dig'].isin([1,5])]\n",
    "\tYout = np.zeros(Xout.shape[0])\n",
    "\tYout[Xout.dig == 1] = 1\n",
    "\tXout = Xout.iloc[:,1:3]\n",
    "\tX = train[train['dig'].isin([1,5])]\n",
    "\tX = X.iloc[0:,1:3]\n",
    "\tgram_out = (1+np.dot(Xout, X.T))**Q\n",
    "\ttest_pred = clf.predict(gram_out)\n",
    "\terror_out = 1-np.sum(np.sign(test_pred) == np.sign(Yout))/test_pred.shape[0]\n",
    "\treturn error_out\n",
    "for c in [0.001,0.01,0.1,1]:\n",
    "\tfit = one_vs_one(1,5,train,c,2)\n",
    "\tclf = fit\n",
    "\terror = error_compute(clf,train,train,2)\n",
    "\terr.append(error)\n",
    "\tsv.append(np.sum(clf.n_support_))\n",
    "\t# Eout compute\n",
    "\terror_out = error_compute(clf,train,test,2)\n",
    "\terr_out.append(error_out)\n",
    "print('in sample error',err)\n",
    "print('number of supporter vectors',sv)\n",
    "print('out of sample error', err_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. As shown above. $E_{in}$, $E_{out}$ and number of support vectors are not strictly going up/down.\n",
    "The only right option is maximum C achieves the lowest $E_{in}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Q=2 Ein  Q=2 Eout   Q=5 Ein  Q=5 Eout\n",
      "0  0.008969  0.016509  0.004484  0.018868\n",
      "1  0.004484  0.016509  0.004484  0.021226\n",
      "2  0.004484  0.018868  0.003844  0.021226\n",
      "3  0.003203  0.018868  0.003203  0.021226\n",
      "   Q=2 n_sv  Q=5 n_sv\n",
      "0     236.0      26.0\n",
      "1      76.0      25.0\n",
      "2      34.0      23.0\n",
      "3      24.0      21.0\n"
     ]
    }
   ],
   "source": [
    "error = np.empty([4,4])\n",
    "sv = np.empty([4,2])\n",
    "i = 0\n",
    "for c in [0.0001,0.001,0.01,1]:\n",
    "\tfit2 = one_vs_one(1,5,train,c,2)\n",
    "\tfit5 = one_vs_one(1,5,train,c,5)\n",
    "\tclf2 = fit2\n",
    "\tclf5 = fit5\n",
    "\tsv[i,:] = np.array([np.sum(clf2.n_support_), np.sum(clf5.n_support_)])\n",
    "\terr2_in = error_compute(clf2, train, train,2)\n",
    "\terr5_in = error_compute(clf5, train, train,5)\n",
    "\terr2_out = error_compute(clf2, train, test,2)\n",
    "\terr5_out = error_compute(clf5, train, test,5)\n",
    "\terror[i,:] = np.array([err2_in,err2_out,err5_in,err5_out])\n",
    "\ti += 1\n",
    "error = pd.DataFrame(error)\n",
    "error.columns = ['Q=2 Ein', 'Q=2 Eout', 'Q=5 Ein', 'Q=5 Eout']\n",
    "sv = pd.DataFrame(sv)\n",
    "sv.columns = ['Q=2 n_sv','Q=5 n_sv']\n",
    "print(error)\n",
    "print(sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6, as shown above. C=0.0001, $E_{in}$ is lower at Q=5  \n",
    "C = 0.001, n_support_vector is lower at Q=5  \n",
    "C = 0.01, $E_{in}$ is lower at Q=5  \n",
    "C = 1, $E_{out}$ is higher at Q=5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "def cross_val(train, tar1, tar2, k, c, Q):\n",
    "\t# step1 k fold split\n",
    "\tkf = KFold(n_splits = k, shuffle = True)\n",
    "\tX_use = train[train['dig'].isin([tar1,tar2])]\n",
    "\terror = np.empty(k)\n",
    "\ti = 0\n",
    "\tfor train_index, test_index in kf.split(X_use): \n",
    "\t# X_use is dig(tar1 or tar2), par1, par2\n",
    "\t\tXtrain = X_use.iloc[train_index,:]\n",
    "\t\tXtest = X_use.iloc[test_index,:]\n",
    "\t\tclf = one_vs_one(tar1,tar2,Xtrain,c,Q)\n",
    "\t\terr = error_compute(clf,Xtrain,Xtest,Q)\n",
    "\t\terror[i] = err\n",
    "\t\ti += 1\n",
    "\treturn np.mean(err)\n",
    "\n",
    "s = []\n",
    "Cs = [0.0001,0.001,0.01,0.1,1]\n",
    "for i in range(0,100):\n",
    "\terror = np.empty(5)\n",
    "\tj = 0\n",
    "\tfor c in Cs:\n",
    "\t\terr = cross_val(train, 1, 5, 10, c, 2)\n",
    "\t\terror[j] = err\n",
    "\t\tj += 1\n",
    "\ts.append(np.argmin(error))\n",
    "counts = np.bincount(np.array(s))\n",
    "print(Cs[np.argmax(counts)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. As shown here. I use KFold function to split the training set into training set and validation set, fold = 10, randomly. The most frequent winner is 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00480769230769\n"
     ]
    }
   ],
   "source": [
    "error = np.empty(100)\n",
    "for i in range(0,100):\n",
    "\terr = cross_val(train, 1, 5, 10, 0.001, 2)\n",
    "\terror[i] = err\n",
    "print(np.mean(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. As shown here, the cross validation error is closes to 0.005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00384369  0.0044843   0.00320307  0.00256246  0.00128123]\n"
     ]
    }
   ],
   "source": [
    "Xin_use = train[train['dig'].isin([1,5])]\n",
    "Xout_use = test[test['dig'].isin([1,5])]\n",
    "Yin = np.zeros(Xin_use.shape[0])\n",
    "Yout = np.zeros(Xout_use.shape[0])\n",
    "Yin[Xin_use.dig == 1] = 1\n",
    "Yout[Xout_use.dig == 1] =1\n",
    "Cs = [0.01,1,100, 10**4, 10**6]\n",
    "Ein = []\n",
    "Xin = Xin_use.iloc[:,1:3]\n",
    "Xout = Xout_use.iloc[:,1:3]\n",
    "for c in Cs:\n",
    "\tclf = svm.SVC(C=c,kernel='rbf')\n",
    "\tclf.fit(Xin, Yin)\n",
    "\tY_pred = np.sign(clf.predict(Xin))\n",
    "\tEin.append(accuracy_score(np.sign(Yin),Y_pred))\n",
    "print(1-np.array(Ein))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9, as shown above, the larges C gives us the lowest $E_{in}$, which make sense in that the rbf is in the infinite dimensions, so the more strict the error-tolerance (c) is, the higher effective dimension it will reach, therefore producing a lower in-sample error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02122642  0.02122642  0.01886792  0.01886792  0.02122642]\n"
     ]
    }
   ],
   "source": [
    "Eout = []\n",
    "for c in Cs:\n",
    "\tclf = svm.SVC(C=c,kernel='rbf')\n",
    "\tclf.fit(Xin, Yin)\n",
    "\tY_pred = np.sign(clf.predict(Xout))\n",
    "\tEout.append(accuracy_score(np.sign(Yout),Y_pred))\n",
    "print(1-np.array(Eout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. As shown above. C=100 gives the best Eout. As reasoned before, the more strict C is, the higher dimension the model will reach, and consequently increases the risk of overfitting. Since SVM is good at self controlling overfitting, so the difference is not very big.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
